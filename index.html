<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <title>SpeechJudge</title>
  <script src="./static/tailwind.js"></script>
  <script src="./static/vue.global.js"></script>
  <script src="./static/flowbite.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/papaparse@5.3.0/papaparse.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <style type="text/tailwindcss">
    @tailwind base;
    @tailwind components;
    @tailwind utilities;

    @layer base {
      a[href^="http"] {
        @apply text-blue-500;
      }

      a:hover {
        @apply text-blue-700 underline;
      }

      h1 {
        @apply mb-6 text-3xl font-bold text-gray-900 text-center;
      }

      h2 {
        @apply mb-3 pt-4 text-2xl font-bold tracking-tight text-gray-900;
      }

      h4 {
        @apply mb-4 pt-12 text-3xl font-semibold text-gray-900;
      }

      h5 {
        @apply mb-3 pt-4 text-2xl font-bold tracking-tight text-gray-900;
      }

      h6 {
        @apply mb-3 pt-4 text-xl font-bold tracking-tight text-gray-900;
      }

      p {
        @apply font-normal text-gray-900 mt-0 mb-0;
      }
    }

    @layer components {
      .list-select {
        @apply block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0;
      }

      .tooltip {
        position: absolute;
        text-align: center;
        padding: 8px;
        font: 12px sans-serif;
        background: lightsteelblue;
        border: 0;
        border-radius: 8px;
        pointer-events: none;
        opacity: 0;
      }

      .text-p {
        @apply mb-4 font-normal text-gray-900 leading-relaxed;
      }

      .author {
        @apply font-medium mr-2 pb-1 text-gray-900 inline-block text-base;
      }

      .author a {
          @apply text-gray-900;
        }

      .affiliation {
        @apply font-medium mr-2 pb-1 text-gray-900 block text-base;
      }
    }
  </style>

  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {}
        }
      }
    }
  </script>
</head>

<body>
  <div id="app">

    <nav class="bg-white fixed w-full z-20 top-0 start-0 border-gray-200 border-b border-gray-200"
      style="font-weight: bold;">
      <div class="max-w-screen-xl flex flex-wrap items-center justify-between mx-auto p-3">
        <button data-collapse-toggle="navbar-default" type="button"
          class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200"
          aria-controls="navbar-default" aria-expanded="false">
          <span class="sr-only">Open main menu</span>
          <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14">
            <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"
              d="M1 1h15M1 7h15M1 13h15" />
          </svg>
        </button>

        <div class="hidden w-full md:block md:w-auto ml-auto" id="navbar-default">
          <ul
            class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50 md:flex-row md:space-x-8 rtl:space-x-reverse md:mt-0 md:border-0 md:bg-white">
            <li>
              <a href="#home" class="list-select"><b>Home</b></a>
            </li>
            <li>
              <a href="#data" class="list-select"><b>SpeechJudge-Data</b></a>
            </li>
            <li>
              <a href="#eval" class="list-select"><b>SpeechJudge-Eval</b></a>
            </li>
            <li>
              <a href="#grm" class="list-select"><b>SpeechJudge-GRM</b></a>
            </li>
            <li>
              <a href="#applications" class="list-select"><b>Applications</b></a>
            </li>
          </ul>
        </div>
      </div>
    </nav>


    <div class="p-4 mt-24 max-w-screen-xl mx-auto" id="home">
      <div class="mt-16 mb-8">
        <h1>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</h1>
      </div>

      <div class="max-w-screen-xl mx-auto mt-8 mb-16">
        <div class="text-center h-2 flex flex-col gap-3">
        </div>

        <div class="max-w-[900px] flex flex-col mx-auto mt-8 mb-16">
          <h2>Abstract</h2>
          <p class="text-p">
            Aligning large generative models with human feedback is a critical challenge. In
            speech synthesis, this is particularly pronounced due to the lack of a large-scale
            human preference dataset, which hinders the development of models that truly
            align with human perception. To address this, we introduce <b><i>SpeechJudge</i></b>, a comprehensive suite
            comprising a dataset, a benchmark, and a reward model centered
            on naturalnessâ€”one of the most fundamental subjective metrics for speech synthesis. First, we present
            <b><i>SpeechJudge-Data</i></b>, a large-scale human feedback corpus
            of 99K speech pairs. The dataset is constructed using a diverse set of advanced
            zero-shot text-to-speech (TTS) models across diverse speech styles and multiple
            languages, with human annotations for both intelligibility and naturalness preference. From this, we
            establish <b><i>SpeechJudge-Eval</i></b>, a challenging benchmark for
            speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this
            task; the best-performing model, Gemini-2.5-Flash,
            achieves less than 70% agreement with human judgment, highlighting a significant
            gap for improvement. To bridge this gap, we develop <b><i>SpeechJudge-GRM</i></b>,
            a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on
            SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning
            (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning
            (RL) with GRPO on challenging cases. On the <b><i>SpeechJudge-Eval</i></b> benchmark,
            the proposed <b><i>SpeechJudge-GRM</i></b> demonstrates superior performance, achieving
            77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic
            Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be
            also employed as a reward function during the post-training of speech generation
            models to facilitate their alignment with human preferences.
          </p>
        </div>


        <div class="mt-8 mb-8" id="data">
          <h4>SpeechJudge-Data</h4>
          <p>
            We recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two
            fundamental speech aspects: <b><i>intelligibility</i></b> and <b><i>naturalness</i></b>. The human
            annotators are instructed to
            perform two tasks based on a speech pair: <b>(a)</b> pointwise annotation of text
            accuracy to assess intelligibility, and <b>(b)</b> pairwise preference annotation to judge relative speech
            naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs,
            with each pair receiving an average of 2.49 annotations from different labelers.

            The system interface for human annotation is shown below:
          </p>

          <img class="py-8 max-w-screen-md mx-auto" src="img/system-ui.png" alt="Annotation System UI">

          <p class="mt-4">
            For data synthesis, we employ a diverse set of advanced open-source <b>zero-shot TTS models</b> with
            varying
            architectures (see the subfigure (a)) to generate paired speech samples (see
            the subfigure (d)). We prepare speech references in both <b>regular</b>
            and <b>expressive</b>
            styles sourced from several datasets (see the subfigure (b)). In addition, we construct
            multilingual target texts encompassing both <b>monolingual</b> and <b>cross-lingual</b> synthesis scenarios
            to ensure data diversity (see the subfigure (c)).
          </p>

          <img class="py-8 max-w-screen-lg mx-auto" src="img/data_distribution.png" alt="Data Distribution">

        </div>

        <div class="mt-8 mb-8" id="eval">
          <h4>SpeechJudge-Eval</h4>
          <p>
            We design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is
            structured as follows: <b><i>given a target text and two corresponding speech samples, a model needs to
                judge
                which one is more natural</i></b>. To construct the evaluation set, we select a subset from the
            SpeechJudge-Data
            where human annotators demonstrated <b>high inter-annotator agreement</b>, ensuring a high-quality ground
            truth.</b>
          </p>

          <img class="py-8 w-3/5 mx-auto" src="img/eval_benchmark.png" alt="Evaluation Benchmark">

          <p>
            We benchmark the naturalness judgment capability of various models based on SpeechJudge-Eval. A key
            observation is that speech naturalness judgment is a highly challenging task. The best-performing model,
            <a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash">Gemini-2.5-Flash</a>, still only
            achieves less than 70% agreement with human preferences.
          </p>
        </div>

        <div class="mt-8" id="grm">
          <h4>SpeechJudge-GRM</h4>
          <p class="mb-2">
            Based on the proposed SpeechJudge-Data, we further explore how to train a <b>Generative Reward Model
              (GRM)</b>
            capable of accurately capturing human preferences. To this end, we introduce <b><i>SpeechJudge-GRM</i></b>,
            a model built upon <a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B">Qwen2.5-Omni-7B (Thinker)</a>, and
            trained
            through a
            two-stage
            <i>"Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)"</i> post-training process specially designed
            for
            speech naturalness judgment task.
          </p>

          <img class="py-8 max-w-screen-md mx-auto" src="img/grm.png" alt="GRM Architecture">

          <p>
            The results of SpeechJudge-GRM on the SpeechJudge-Eval benchmark are shown below. We develop
            <b><i>SpeechJudge-BTRM</i></b> as a baseline, which utilizes the classic Bradley-Terry Reward Model (BTRM)
            paradigm by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction.
            We can
            observe that the SpeechJudge-GRM reaches 77.2% of agreement with human preferences, outperforming the
            classic BTRM (72.7%). Furthermore, when using the <b>inference-time scaling</b>
            (voting@10), the performance of SpeechJudge-GRM further improves to 79.4%.
          </p>

          <img class="py-8 w-1/3 mx-auto" src="img/grm_results.png" alt="GRM Results">

          <p>
            In addition to exhibiting superior performance in naturalness judgment, SpeechJudge-GRM also demonstrates a
            certain degree of <b>explainability</b> by revealing its <b>chain-of-thought reasoning</b> process. Below
            are several
            cases illustrating the reasoning behavior of SpeechJudge-GRM, randomly selected from the SpeechJudge-Eval
            benchmark:
          </p>

          <p>
            <br>
          </p>

          <div class="flex flex-col mx-auto mb-16 p-6 bg-gray-50 rounded-lg">
            <div v-if="eval_data.length > 0">
              <!-- Case selection buttons -->
              <ul class="flex flex-wrap justify-center mb-6">
                <li v-for="(item, index) in eval_data" :key="index" class="me-2 mb-2">
                  <a href="#" @click.prevent="selectGrmEntry(index)"
                    :class="{'bg-gray-900 text-white hover:text-white': currentGrmIndex === index, 'bg-gray-200 text-gray-800 hover:text-gray-800 hover:bg-gray-300': currentGrmIndex !== index}"
                    class="inline-flex items-center justify-center min-w-[120px] px-4 py-2.5 rounded-lg transition-all duration-200 font-medium">
                    <span class="relative">Case {{ index + 1 }}</span>
                  </a>
                </li>
              </ul>

              <!-- User Prompt Section -->
              <div class="flex flex-row">
                <span class="font-bold text-base text-right pr-4 w-1/6">Prompt</span>
                <div class="w-5/6">

                  <!-- <p>A: {{ currentGrmEntry.audioA_spell_wrong }}</p>
                  <p>B: {{ currentGrmEntry.audioB_spell_wrong }}</p>
                  <p>Audio A: {{ currentGrmEntry.audioA }}</p>
                  <p>Audio B: {{ currentGrmEntry.audioB }}</p> -->

                  <p class="mb-2">
                    We are comparing the naturalness of two text-to-speech models' outputs. The models need to speak the
                    target text accurately and naturally. </p>
                  <p class="mb-2">Target text: <b><i>{{ currentGrmEntry.target_text }}</i></b></p>

                  <span class="mb-2">Output A:</span>
                  <play-audio :key="currentGrmEntry.audioA" :src="currentGrmEntry.audioA"></play-audio>
                  <span class="mb-2">Output B:</span>
                  <play-audio :key="currentGrmEntry.audioB" :src="currentGrmEntry.audioB"></play-audio>
                  <p class="mt-2 mb-2">
                    Analyze the two output above, and score them with number from 1 to 10.</p>
                  <p> Note:</p>
                  <ul class="list-disc list-inside">
                    <li class="me-2">
                      Please evaluate the naturalness of both audio outputs based on the following criteria:
                      Prosody and Intonation, Pacing and Rhythm, Articulation and Clarity, and
                      Overall Naturalness.
                    </li>
                    <li class="me-2">After conducting a detailed analysis of each criterion, using the following
                      output template to
                      highlight your conclusion: Output A: X, Output B: X.</li>
                  </ul>

                </div>
              </div>


              <!-- User Prompt Section -->
              <div class="flex flex-row mt-6">
                <span class="font-bold text-base text-right pr-4 w-1/6">Human Preference</span>
                <div class="w-5/6">
                  <p class="mb-2"><b>{{ currentGrmEntry.naturalness_result }}</b> is better.</p>
                </div>
              </div>

              <!-- Completion Section -->
              <div class="flex flex-row mt-6">
                <div class="flex flex-col items-end pr-4 w-1/6">
                  <span class="font-bold text-base text-right">SpeechJudge-GRM</span>
                  <!-- Completion selection buttons -->
                  <ul class="flex flex-col gap-2 mt-3 text-sm">
                    <li class="me-2">
                      <a href="#" @click.prevent="selectCompletion(0)"
                        :class="{'bg-blue-50 text-blue-600 ring-1 ring-blue-600/20': currentCompletionIndex === 0, 'text-gray-600 hover:bg-gray-50 hover:text-gray-900': currentCompletionIndex !== 0}"
                        class="inline-flex items-center justify-center min-w-[100px] px-3 py-1.5 rounded-lg transition-all duration-200 text-sm">
                        <span class="relative">Output 1</span>
                      </a>
                    </li>
                    <li class="me-2">
                      <a href="#" @click.prevent="selectCompletion(1)"
                        :class="{'bg-blue-50 text-blue-600 ring-1 ring-blue-600/20': currentCompletionIndex === 1, 'text-gray-600 hover:bg-gray-50 hover:text-gray-900': currentCompletionIndex !== 1}"
                        class="inline-flex items-center justify-center min-w-[100px] px-3 py-1.5 rounded-lg transition-all duration-200 text-sm">
                        <span class="relative">Output 2</span>
                      </a>
                    </li>
                    <li class="me-2">
                      <a href="#" @click.prevent="selectCompletion(2)"
                        :class="{'bg-blue-50 text-blue-600 ring-1 ring-blue-600/20': currentCompletionIndex === 2, 'text-gray-600 hover:bg-gray-50 hover:text-gray-900': currentCompletionIndex !== 2}"
                        class="inline-flex items-center justify-center min-w-[100px] px-3 py-1.5 rounded-lg transition-all duration-200 text-sm">
                        <span class="relative">Output 3</span>
                      </a>
                    </li>
                  </ul>
                </div>
                <div class="w-5/6 text-gray-800">
                  <div v-html="currentCompletionHtml" class="gap-2 flex flex-col"></div>
                </div>
              </div>

            </div>
            <div v-else class="text-center text-gray-500">
              Loading SpeechJudge-GRM data...
            </div>
          </div>
        </div>

        <div class="mt-8" id="applications">
          <h4>Applications</h4>

          <h5 class="mb-3 pt-4 text-2xl font-bold tracking-tight text-gray-900">High-Quality Sample Selection based on
            SpeechJudge-GRM</h5>
          <p class="mb-2">
            We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the
            hard cases from <a href="https://github.com/BytedanceSpeech/seed-tts-eval">SeedTTS-Eval</a> and the
            code-switching cases from <a
              href="https://huggingface.co/datasets/amphion/Amphion-TTS-Eval">Amphion-TTS-Eval</a> as target texts. For
            each text, we instruct the Qwen2.5-Omni-7B (Talker) to generate 100 speeches. We then ask
            human subjects to compare the <b>best-of-100</b> outputâ€”as selected by either SpeechJudge-BTRM or
            SpeechJudge-GRMâ€”against a randomly sampled output. Here are some examples:
            <br>
            <br>
          </p>
          <div class="relative overflow-x-auto shadow-md sm:rounded-lg mb-8">
            <table class="w-full text-sm text-left rtl:text-right min-w-[800px]">
              <thead class="text-sm text-gray-900 font-bold bg-gray-50">
                <tr>
                  <th scope="col" class="px-6 py-3 w-[40%]">Target Text</th>
                  <th scope="col" class="px-6 py-3 w-[20%]">Random</th>
                  <th scope="col" class="px-6 py-3 w-[20%]">SpeechJudge-BTRM</th>
                  <th scope="col" class="px-6 py-3 w-[20%]">SpeechJudge-GRM</th>
                </tr>
              </thead>
              <tbody>
                <template v-for="(item, index) in qwen_select_data" :key="index"
                  class="bg-white border-b hover:bg-gray-50">
                  <tr>
                    <td>{{ item.target_text }}</td>
                    <td>
                      <play-audio :src="item.random"></play-audio>
                    </td>
                    <td>
                      <play-audio :src="item.btrm"></play-audio>
                    </td>
                    <td>
                      <play-audio :src="item.grm"></play-audio>
                    </td>
                  </tr>
                </template>
              </tbody>
            </table>
          </div>

          <h5 class="mb-3 pt-4 text-2xl font-bold tracking-tight text-gray-900">Post-Training of Zero-Shot TTS based on
            SpeechJudge-GRM
          </h5>
          <p class="mb-2">
            We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model.
            Specifically, we develop a new zero-shot TTS model, <b><i>Qwen2.5-0.5B-TTS</i></b>, to serve as the base
            model,
            which was not involved in the construction of the SpeechJudge-Data. The compared methods are as follows:
          <ul class="list-disc pl-6">
            <li><b>Qwen2.5-0.5B-TTS (Base):</b> The base model without any post-training. Based on it, we use the
              different methods to perform post-training:</li>
            <li class="list-[circle] list-inside pl-6 mt-1"><b>w/ SpeechJudge-Data:</b> We use the SpeechJudge-Data to
              perform <a href="https://arxiv.org/abs/2305.18290">the offline DPO alignment</a>.</li>
            <li class="list-[circle] list-inside pl-6 mt-1"><b>w/ SpeechJudge-GRM (offline):</b> We use SpeechJudge-GRM
              as an offline preference data annotator. We take all speech pairs from <a
                href="https://huggingface.co/datasets/amphion/INTP">the INTP dataset</a> and reannotate
              their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on
              the resulting data.</li>
            <li class="list-[circle] list-inside pl-6 mt-1"><b>w/ SpeechJudge-GRM (online):</b> We use SpeechJudge-GRM
              as a reward
              function for <a href="https://arxiv.org/abs/2402.04792">the online DPO algorithm</a>. The training data
              consists of only the
              prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</li>
          </ul>
          <br>
          <br>
          </p>
          <div class="relative overflow-x-auto shadow-md sm:rounded-lg mb-8">
            <table class="w-full text-sm text-left rtl:text-right min-w-[800px]">
              <thead class="text-sm text-gray-900 font-bold bg-gray-50">
                <tr>
                  <th scope="col" class="px-6 py-3 w-[15%]">Reference</th>
                  <th scope="col" class="px-6 py-3 w-[25%]">Target Text</th>
                  <th scope="col" class="px-6 py-3 w-[15%]">Base</th>
                  <th scope="col" class="px-6 py-3 w-[15%]">w/ SpeechJudge-Data</th>
                  <th scope="col" class="px-6 py-3 w-[15%]">w/ SpeechJudge-GRM (offline)</th>
                  <th scope="col" class="px-6 py-3 w-[15%]">w/ SpeechJudge-GRM (online)</th>
                </tr>
              </thead>
              <tbody>
                <template v-for="(item, index) in tts_enhence_data" :key="index">
                  <tr class="bg-white border-b hover:bg-gray-50">
                    <td>
                      <play-audio :src="item.prompt"></play-audio>
                    </td>
                    <td>{{ item.target_text }}</td>
                    <td>
                      <play-audio :src="item.base"></play-audio>
                    </td>
                    <td>
                      <play-audio :src="item.data"></play-audio>
                    </td>
                    <td>
                      <play-audio :src="item.offline"></play-audio>
                    </td>
                    <td>
                      <play-audio :src="item.online"></play-audio>
                    </td>
                  </tr>
                </template>
              </tbody>
            </table>
          </div>


        </div>
      </div>

    </div>
  </div>

  <script>
    const { createApp, ref, onMounted, computed } = Vue

    const PlayAudio = {
      props: ['src'],
      template: `
        <audio controls class="w-full">
          <source :src="encodeURIComponent(src)" type="audio/wav">
          Your browser does not support the audio element.
        </audio>
      `,
      methods: {
        encodeURIComponent: encodeURIComponent
      }
    }

    createApp({
      components: {
        'play-audio': PlayAudio
      },
      setup() {

        const tts_enhence_data = ref([]);
        const qwen_select_data = ref([]);
        const eval_data = ref([]);
        const currentGrmIndex = ref(0);
        const currentCompletionIndex = ref(0);

        const currentGrmEntry = computed(() => {
          return eval_data.value[currentGrmIndex.value] || {};
        });

        const currentCompletionHtml = computed(() => {
          const entry = currentGrmEntry.value;
          switch (currentCompletionIndex.value) {
            case 0: return entry.completions1_html;
            case 1: return entry.completions2_html;
            case 2: return entry.completions3_html;
            default: return '';
          }
        });

        const selectGrmEntry = (index) => {
          currentGrmIndex.value = index;
          currentCompletionIndex.value = 0; // Reset completion index when changing entry
        };

        const selectCompletion = (index) => {
          currentCompletionIndex.value = index;
        };

        onMounted(() => {
          Papa.parse('SpeechJudge_subeval/tts_enhence.csv', {
            download: true,
            header: true,
            complete: (results) => {
              tts_enhence_data.value = results.data;
            },
            error: (error) => {
              console.error('Error fetching or parsing tts_enhence.csv:', error);
            }
          });

          Papa.parse('SpeechJudge_subeval/qwen_select.csv', {
            download: true,
            header: true,
            complete: (results) => {
              qwen_select_data.value = results.data;
            },
            error: (error) => {
              console.error('Error fetching or parsing qwen_select.csv:', error);
            }
          });

          Papa.parse('SpeechJudge_eval/eval.csv', {
            download: true,
            header: true,
            complete: (results) => {
              eval_data.value = results.data.map(item => {
                return {
                  ...item,
                  completions1_html: marked.parse(item.completions1 || ''),
                  completions2_html: marked.parse(item.completions2 || ''),
                  completions3_html: marked.parse(item.completions3 || '')
                };
              }).filter(item => item.target_text); // Filter out empty rows
            },
            error: (error) => {
              console.error('Error fetching or parsing eval.csv:', error);
            }
          });

        });

        return {
          tts_enhence_data,
          qwen_select_data,
          eval_data,
          currentGrmIndex,
          currentCompletionIndex,
          currentGrmEntry,
          currentCompletionHtml,
          selectGrmEntry,
          selectCompletion
        }
      }
    }).mount('#app')
  </script>

</body>

</html>